# -*- coding: utf-8 -*-
"""Smart ChatBot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Da1WJbGA20Gj2_kMOWaqpi9nGnnsW-Hn
"""

import numpy as np
import nltk
import string
import random

f = open('/content/chat.txt','r',errors = 'ignore')
raw_doc = f.read()
raw_doc



raw_doc = raw_doc.lower()
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

raw_doc = raw_doc.lower()
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
sentence_tokens = nltk.sent_tokenize(raw_doc)
word_tokens = nltk.word_tokenize(raw_doc)

lemmer = nltk.stem.WordNetLemmatizer()
def LemTokens(tokens):
  return [lemmer.lemmatize(token) for token in tokens]
remove_punkt_dict = dict((ord(punkt), None ) for punkt in string.punctuation)
def LemNormalize(text):
  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punkt_dict)))

greet_inputs = ('hello', 'hi', 'wassup', 'how are you ?')
greet_responses = ('hi', 'hey', 'hey there !', 'there there !!' )
def greet(sentence):
  for word in sentence.split():
    if word.lower in greet_inputs:
      return random.choice(greet_responses)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def response(user_responce):
  robo1_response = ''
  TfidfVec = TfidfVectorizer(tokenizer = LemNormalize, stop_words = 'english')
  tfidf = TfidfVec.fit_transform(sentence_tokens)
  vals = cosine_similarity(tfidf[-1], tfidf)
  idx = vals.argsort()[0][-2]
  flat = vals.flatten()
  flat.sort()
  req_tfidf = flat[-2]
  if (req_tfidf == 0):
    robo_response = robo1_response + "I am Sorry! Unable to understand you."
    return robo1_response
  else:
    robo1_response = robo1_response+ sentence_tokens[idx]
    return robo1_response

flag = True
print('Hello! I am your intelligent bot. Ask anything by just typingafter you greet me. If there is no further queries, type BYE!')
while(flag == True):
  user_response = input()
  user_response = user_response.lower()
  if(user_response != 'BYE!'):
    if(user_response == 'Thank You' or user_response == 'thanks'):
      flag = False
      print('You are welcome....')
    else:
      sentence_tokens.append(user_response)
      word_tokens = word_tokens + nltk.word_tokenize(user_response)
      final_word = list(set(word_tokens))
      print('', end = '')
      print(response(user_response))
      sentence_tokens.remove(user_response)
  else:
    flag = False
    print('Goodbye!')

